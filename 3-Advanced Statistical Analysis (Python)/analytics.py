# Analyse Statistique Avanc√©e des March√©s Publics Marocains
# Version am√©lior√©e avec graphiques sans chevauchement

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import warnings
warnings.filterwarnings('ignore')

# Configuration matplotlib pour l'affichage fran√ßais
plt.rcParams['font.size'] = 11
plt.rcParams['figure.figsize'] = (15, 10)
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")
sns.set_palette("husl")

class AnalyseMarchesPublicsAmeliore:
    """Classe am√©lior√©e pour l'analyse des march√©s publics avec graphiques optimis√©s"""
    
    def __init__(self, data_path=None, df=None):
        """Initialisation avec donn√©es"""
        if df is not None:
            self.df = df.copy()
        elif data_path:
            self.df = pd.read_excel(data_path)
        else:
            raise ValueError("Fournir soit data_path soit df")
        
        self.df_encoded = None
        self.pca = None
        self.scaler = None
        self.label_encoders = {}
        
    def preprocess_data(self):
        """Pr√©paration et encodage des donn√©es"""
        print("=== PR√âPARATION DES DONN√âES ===")
        
        # Nettoyage des donn√©es
        self.df['R√©f√©rence'] = pd.to_datetime(self.df['R√©f√©rence'], errors='coerce')
        self.df['Publi√© le'] = pd.to_datetime(self.df['Publi√© le'], errors='coerce')
        
        # Cr√©ation de variables d√©riv√©es
        self.df['Ann√©e'] = self.df['R√©f√©rence'].dt.year
        self.df['Mois'] = self.df['R√©f√©rence'].dt.month
        self.df['D√©lai_publication'] = (self.df['R√©f√©rence'] - self.df['Publi√© le']).dt.days
        
        # Variables pour l'analyse
        categorical_vars = ['Proc√©dure', 'Cat√©gorie', 'Type de d√©pense personnalis√©', 
                           'region', 'Acheteur public']
        
        # Encodage des variables cat√©gorielles
        df_for_analysis = self.df.copy()
        
        for var in categorical_vars:
            if var in df_for_analysis.columns:
                le = LabelEncoder()
                df_for_analysis[f'{var}_encoded'] = le.fit_transform(
                    df_for_analysis[var].fillna('Inconnu').astype(str)
                )
                self.label_encoders[var] = le
        
        # S√©lection des variables pour l'analyse
        analysis_vars = [
            'Proc√©dure_encoded', 'Cat√©gorie_encoded', 'Type de d√©pense personnalis√©_encoded',
            'region_encoded', 'Procedure_AOO', 'Procedure_AOS', 'Procedure_CONCA',
            'Procedure_APPEL', 'Procedure_AVIS', 'Categorie_Services',
            'Categorie_Travaux', 'Categorie_Fournitures', 'Ann√©e', 'Mois'
        ]
        
        # Filtrer les variables existantes
        analysis_vars = [var for var in analysis_vars if var in df_for_analysis.columns]
        
        self.df_encoded = df_for_analysis[analysis_vars].fillna(0)
        
        print(f"‚úÖ Donn√©es pr√©par√©es: {self.df_encoded.shape[0]} observations, {self.df_encoded.shape[1]} variables")
        
        return self.df_encoded
    
    def plot_pca_scree(self, var_explained, cumvar_explained):
        """Graphique d'√©boulis des valeurs propres - Version am√©lior√©e"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. √âboulis des valeurs propres
        n_components = len(var_explained)
        x_pos = range(1, n_components + 1)
        
        bars = ax1.bar(x_pos, var_explained, alpha=0.7, color='steelblue', 
                      edgecolor='navy', linewidth=1.2)
        line = ax1.plot(x_pos, var_explained, 'ro-', linewidth=2.5, 
                       markersize=8, color='darkred')
        
        # Ajouter les valeurs sur les barres
        for i, (x, y) in enumerate(zip(x_pos, var_explained)):
            if i < 5:  # Afficher seulement les 5 premi√®res pour √©viter l'encombrement
                ax1.text(x, y + 0.005, f'{y:.3f}', ha='center', va='bottom', 
                        fontweight='bold', fontsize=10)
        
        ax1.set_xlabel('Composantes Principales', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Variance Expliqu√©e', fontsize=12, fontweight='bold')
        ax1.set_title('üìä √âboulis des Valeurs Propres\n(M√©thode du Coude)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax1.grid(True, alpha=0.3)
        ax1.set_xticks(x_pos)
        
        # 2. Variance cumul√©e
        line_cum = ax2.plot(x_pos, cumvar_explained, 'go-', linewidth=3, 
                           markersize=10, color='forestgreen', label='Variance cumul√©e')
        ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.8, linewidth=2,
                   label='Seuil 80%')
        ax2.axhline(y=0.9, color='orange', linestyle='--', alpha=0.8, linewidth=2,
                   label='Seuil 90%')
        
        # Marquer le point 80%
        idx_80 = np.where(cumvar_explained >= 0.8)[0]
        if len(idx_80) > 0:
            first_80 = idx_80[0]
            ax2.scatter(first_80 + 1, cumvar_explained[first_80], 
                       s=150, color='red', zorder=5)
            ax2.annotate(f'CP{first_80 + 1}\n{cumvar_explained[first_80]:.1%}', 
                        xy=(first_80 + 1, cumvar_explained[first_80]),
                        xytext=(first_80 + 1.5, cumvar_explained[first_80] - 0.1),
                        arrowprops=dict(arrowstyle='->', color='red'),
                        fontsize=10, fontweight='bold')
        
        ax2.set_xlabel('Nombre de Composantes', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Variance Expliqu√©e Cumul√©e', fontsize=12, fontweight='bold')
        ax2.set_title('üìà Variance Expliqu√©e Cumul√©e\n(Crit√®re de Kaiser)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax2.legend(loc='lower right')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 1.05)
        ax2.set_xticks(x_pos)
        
        plt.tight_layout()
        plt.show()
        
        # Interpr√©tation textuelle
        print(f"\nüìä INTERPR√âTATION DE L'ACP:")
        print(f"   ‚Ä¢ Nombre de composantes pour 80% de variance: {sum(cumvar_explained < 0.8) + 1}")
        print(f"   ‚Ä¢ Nombre de composantes pour 90% de variance: {sum(cumvar_explained < 0.9) + 1}")
        print(f"   ‚Ä¢ Variance de la 1√®re composante: {var_explained[0]:.1%}")
        print(f"   ‚Ä¢ Variance des 3 premi√®res composantes: {sum(var_explained[:3]):.1%}")
    
    def plot_pca_projections(self, X_pca, var_explained):
        """Projections des individus et cercle des corr√©lations - Version am√©lior√©e"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        
        # 1. Projection des march√©s publics
        scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6, s=60, 
                             c=range(len(X_pca)), cmap='viridis', edgecolors='black', linewidth=0.5)
        
        ax1.set_xlabel(f'CP1 ({var_explained[0]:.1%} de variance)', fontsize=12, fontweight='bold')
        ax1.set_ylabel(f'CP2 ({var_explained[1]:.1%} de variance)', fontsize=12, fontweight='bold')
        ax1.set_title('üéØ Projection des March√©s Publics\n(Plan Factoriel CP1-CP2)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax1.grid(True, alpha=0.3)
        
        # Ajouter des lignes de r√©f√©rence
        ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)
        
        # Statistiques sur la projection
        x_range = X_pca[:, 0].max() - X_pca[:, 0].min()
        y_range = X_pca[:, 1].max() - X_pca[:, 1].min()
        ax1.text(0.02, 0.98, f'Dispersion CP1: {x_range:.2f}\nDispersion CP2: {y_range:.2f}', 
                transform=ax1.transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),
                fontsize=10)
        
        # 2. Cercle des corr√©lations (si peu de variables)
        if len(self.df_encoded.columns) <= 15:
            correlations = self.pca.components_[:2].T * np.sqrt(self.pca.explained_variance_[:2])
            
            # Cercle unitaire
            circle = plt.Circle((0, 0), 1, fill=False, color='black', linestyle='--', alpha=0.7, linewidth=2)
            ax2.add_patch(circle)
            
            # Variables
            colors = plt.cm.Set3(np.linspace(0, 1, len(self.df_encoded.columns)))
            
            for i, (var, color) in enumerate(zip(self.df_encoded.columns, colors)):
                # Fl√®che
                ax2.arrow(0, 0, correlations[i, 0], correlations[i, 1], 
                         head_width=0.04, head_length=0.04, fc=color, ec=color, linewidth=2)
                
                # √âtiquette avec fond
                ax2.text(correlations[i, 0]*1.15, correlations[i, 1]*1.15, 
                        var.replace('_encoded', '').replace('_', ' '), 
                        fontsize=9, ha='center', va='center', fontweight='bold',
                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', 
                                 edgecolor=color, alpha=0.8))
            
            ax2.set_xlim(-1.3, 1.3)
            ax2.set_ylim(-1.3, 1.3)
            ax2.set_xlabel(f'CP1 ({var_explained[0]:.1%})', fontsize=12, fontweight='bold')
            ax2.set_ylabel(f'CP2 ({var_explained[1]:.1%})', fontsize=12, fontweight='bold')
            ax2.set_title('üîÑ Cercle des Corr√©lations\n(Contribution des Variables)', 
                         fontsize=14, fontweight='bold', pad=20)
            ax2.grid(True, alpha=0.3)
            ax2.set_aspect('equal')
            
            # Ajouter des lignes de r√©f√©rence
            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)
            
        else:
            # Contributions des variables (pour beaucoup de variables)
            contrib_cp1 = (self.pca.components_[0]**2) / np.sum(self.pca.components_[0]**2) * 100
            contrib_cp2 = (self.pca.components_[1]**2) / np.sum(self.pca.components_[1]**2) * 100
            
            y_pos = np.arange(len(self.df_encoded.columns))
            
            ax2.barh(y_pos - 0.2, contrib_cp1, 0.4, alpha=0.8, label='CP1', color='skyblue')
            ax2.barh(y_pos + 0.2, contrib_cp2, 0.4, alpha=0.8, label='CP2', color='lightcoral')
            
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels([col.replace('_encoded', '').replace('_', ' ') 
                                for col in self.df_encoded.columns], fontsize=9)
            ax2.set_xlabel('Contribution (%)', fontsize=12, fontweight='bold')
            ax2.set_title('üìä Contributions des Variables\n(aux 2 premi√®res CP)', 
                         fontsize=14, fontweight='bold', pad=20)
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def perform_pca(self, n_components=None):
        """Analyse en Composantes Principales avec visualisations am√©lior√©es"""
        print("\n=== ANALYSE EN COMPOSANTES PRINCIPALES (ACP) ===")
        
        if self.df_encoded is None:
            self.preprocess_data()
        
        # Standardisation
        self.scaler = StandardScaler()
        X_scaled = self.scaler.fit_transform(self.df_encoded)
        
        # ACP
        if n_components is None:
            n_components = min(10, self.df_encoded.shape[1])
        
        self.pca = PCA(n_components=n_components)
        X_pca = self.pca.fit_transform(X_scaled)
        
        # Variance expliqu√©e
        var_explained = self.pca.explained_variance_ratio_
        cumvar_explained = np.cumsum(var_explained)
        
        print(f"üìä Variance expliqu√©e par les {n_components} premi√®res composantes:")
        for i, (var, cumvar) in enumerate(zip(var_explained[:5], cumvar_explained[:5])):
            print(f"   CP{i+1}: {var:.3f} ({cumvar:.3f} cumul√©)")
        
        # Visualisations s√©par√©es pour √©viter le chevauchement
        self.plot_pca_scree(var_explained, cumvar_explained)
        self.plot_pca_projections(X_pca, var_explained)
        
        return X_pca
    
    def plot_clustering_analysis(self, X_scaled, inertias, silhouette_scores, k_range):
        """Analyse de clustering avec graphiques s√©par√©s"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # 1. M√©thode du coude
        ax1.plot(k_range, inertias, 'bo-', linewidth=3, markersize=10, color='navy')
        
        # Marquer le coude optimal
        if len(inertias) >= 3:
            optimal_k = self._find_elbow(k_range, inertias)
            optimal_idx = optimal_k - k_range[0]
            ax1.scatter(optimal_k, inertias[optimal_idx], s=200, color='red', 
                       zorder=5, marker='*')
            ax1.annotate(f'Optimal K={optimal_k}', 
                        xy=(optimal_k, inertias[optimal_idx]),
                        xytext=(optimal_k + 0.5, inertias[optimal_idx] + max(inertias)*0.1),
                        arrowprops=dict(arrowstyle='->', color='red', lw=2),
                        fontsize=12, fontweight='bold',
                        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.8))
        
        ax1.set_xlabel('Nombre de Clusters (K)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Inertie (Within-Cluster Sum of Squares)', fontsize=12, fontweight='bold')
        ax1.set_title('üìà M√©thode du Coude\n(D√©termination du K optimal)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax1.grid(True, alpha=0.3)
        ax1.set_xticks(k_range)
        
        # 2. Scores de silhouette
        k_sil = list(k_range)[1:]
        ax2.plot(k_sil, silhouette_scores, 'ro-', linewidth=3, markersize=10, color='darkgreen')
        
        # Marquer le meilleur score
        best_sil_idx = np.argmax(silhouette_scores)
        best_k = k_sil[best_sil_idx]
        ax2.scatter(best_k, silhouette_scores[best_sil_idx], s=200, color='gold', 
                   zorder=5, marker='*')
        ax2.annotate(f'Meilleur Score\nK={best_k}\nScore={silhouette_scores[best_sil_idx]:.3f}', 
                    xy=(best_k, silhouette_scores[best_sil_idx]),
                    xytext=(best_k + 0.5, silhouette_scores[best_sil_idx] - 0.05),
                    arrowprops=dict(arrowstyle='->', color='gold', lw=2),
                    fontsize=11, fontweight='bold',
                    bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        ax2.set_xlabel('Nombre de Clusters (K)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Score de Silhouette', fontsize=12, fontweight='bold')
        ax2.set_title('üéØ Scores de Silhouette\n(Qualit√© des Clusters)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax2.grid(True, alpha=0.3)
        ax2.set_xticks(k_sil)
        
        plt.tight_layout()
        plt.show()
    
    def plot_cluster_results(self, X_scaled, cluster_labels):
        """Visualisation des r√©sultats de clustering"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))
        
        # 1. Projection des clusters
        if self.pca is not None:
            X_pca = self.pca.transform(X_scaled)
            scatter = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, 
                                 cmap='Set1', alpha=0.7, s=80, edgecolors='black', linewidth=0.5)
            ax1.set_xlabel(f'CP1 ({self.pca.explained_variance_ratio_[0]:.1%})', 
                          fontsize=12, fontweight='bold')
            ax1.set_ylabel(f'CP2 ({self.pca.explained_variance_ratio_[1]:.1%})', 
                          fontsize=12, fontweight='bold')
            
            # Centres des clusters
            unique_labels = np.unique(cluster_labels)
            for label in unique_labels:
                mask = cluster_labels == label
                center_x = X_pca[mask, 0].mean()
                center_y = X_pca[mask, 1].mean()
                ax1.scatter(center_x, center_y, s=300, c='black', marker='x', linewidth=4)
                ax1.text(center_x, center_y + 0.3, f'C{label}', ha='center', va='center',
                        fontsize=12, fontweight='bold', color='white',
                        bbox=dict(boxstyle='circle', facecolor='black'))
        else:
            scatter = ax1.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, 
                                 cmap='Set1', alpha=0.7, s=80, edgecolors='black', linewidth=0.5)
            ax1.set_xlabel('Variable 1 (standardis√©e)', fontsize=12, fontweight='bold')
            ax1.set_ylabel('Variable 2 (standardis√©e)', fontsize=12, fontweight='bold')
        
        ax1.set_title('üé® Visualisation des Clusters\n(Projection sur Plan Factoriel)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax1.grid(True, alpha=0.3)
        
        # L√©gende des clusters
        unique_labels = np.unique(cluster_labels)
        for i, label in enumerate(unique_labels):
            ax1.scatter([], [], c=scatter.cmap(scatter.norm(label)), s=100, 
                       label=f'Cluster {label}')
        ax1.legend(title='Clusters', title_fontsize=12, fontsize=11)
        
        # 2. Distribution des clusters
        unique_labels, counts = np.unique(cluster_labels, return_counts=True)
        colors = plt.cm.Set1(np.linspace(0, 1, len(unique_labels)))
        
        bars = ax2.bar(unique_labels, counts, alpha=0.8, color=colors, 
                      edgecolor='black', linewidth=1.5)
        
        # Ajouter les pourcentages et valeurs
        total = len(cluster_labels)
        for i, (bar, label, count) in enumerate(zip(bars, unique_labels, counts)):
            percentage = count / total * 100
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + total * 0.01, 
                    f'{count}\n({percentage:.1f}%)', 
                    ha='center', va='bottom', fontweight='bold', fontsize=11)
        
        ax2.set_xlabel('Cluster', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Nombre de March√©s', fontsize=12, fontweight='bold')
        ax2.set_title('üìä Distribution des Clusters\n(Taille des Groupes)', 
                     fontsize=14, fontweight='bold', pad=20)
        ax2.set_xticks(unique_labels)
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.show()
    
    def _find_elbow(self, k_range, inertias):
        """Trouve le coude dans la courbe d'inertie"""
        if len(inertias) < 3:
            return k_range[0]
        
        # M√©thode de la diff√©rence seconde
        diff1 = np.diff(inertias)
        diff2 = np.diff(diff1)
        
        elbow_idx = np.argmax(diff2) + 2
        return k_range[elbow_idx] if elbow_idx < len(k_range) else k_range[-1]
    
    def perform_clustering(self, method='kmeans', n_clusters_range=(2, 8)):
        """Clustering avec visualisations am√©lior√©es"""
        print("\n=== ANALYSE DE CLUSTERING ===")
        
        if self.df_encoded is None:
            self.preprocess_data()
        
        if self.scaler is None:
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(self.df_encoded)
        else:
            X_scaled = self.scaler.transform(self.df_encoded)
        
        if method == 'kmeans':
            # D√©termination du nombre optimal de clusters
            inertias = []
            silhouette_scores = []
            k_range = range(n_clusters_range[0], n_clusters_range[1] + 1)
            
            print("üîç Recherche du nombre optimal de clusters...")
            for k in k_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                labels = kmeans.fit_predict(X_scaled)
                
                inertias.append(kmeans.inertia_)
                if k > 1:
                    sil_score = silhouette_score(X_scaled, labels)
                    silhouette_scores.append(sil_score)
                    print(f"   K={k}: Inertie={kmeans.inertia_:.2f}, Silhouette={sil_score:.3f}")
            
            # Visualisation de l'analyse du nombre optimal
            self.plot_clustering_analysis(X_scaled, inertias, silhouette_scores, k_range)
            
            # Clustering final avec le nombre optimal
            optimal_k = self._find_elbow(k_range, inertias)
            print(f"üéØ Nombre optimal de clusters: {optimal_k}")
            
            kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
            cluster_labels = kmeans_final.fit_predict(X_scaled)
            
            # Ajout des labels au DataFrame
            self.df['Cluster_KMeans'] = cluster_labels
            
            # Visualisation des r√©sultats
            self.plot_cluster_results(X_scaled, cluster_labels)
            
            # Analyse d√©taill√©e des clusters
            self._analyze_clusters_detailed(cluster_labels, method='kmeans')
            
            return cluster_labels
        
        else:
            raise NotImplementedError("Seule la m√©thode K-means est impl√©ment√©e dans cette version")
    
    def _analyze_clusters_detailed(self, cluster_labels, method):
        """Analyse d√©taill√©e des clusters avec visualisations"""
        print(f"\n=== ANALYSE D√âTAILL√âE DES CLUSTERS ({method.upper()}) ===")
        
        n_clusters = len(np.unique(cluster_labels))
        
        # Analyse par cluster
        cluster_analysis = []
        
        for cluster_id in range(n_clusters):
            mask = cluster_labels == cluster_id
            cluster_data = self.df[mask]
            
            print(f"\nüè∑Ô∏è  CLUSTER {cluster_id}:")
            print(f"   üìä Taille: {mask.sum():,} march√©s ({mask.sum()/len(self.df)*100:.1f}%)")
            
            # Analyse par r√©gion
            if 'region' in cluster_data.columns:
                region_dist = cluster_data['region'].value_counts().head(3)
                print(f"   üåç Top 3 r√©gions:")
                for region, count in region_dist.items():
                    print(f"      ‚Ä¢ {region}: {count} ({count/len(cluster_data)*100:.1f}%)")
            
            # Analyse par cat√©gorie
            if 'Cat√©gorie' in cluster_data.columns:
                cat_dist = cluster_data['Cat√©gorie'].value_counts()
                print(f"   üìÅ R√©partition par cat√©gorie:")
                for cat, count in cat_dist.items():
                    print(f"      ‚Ä¢ {cat}: {count} ({count/len(cluster_data)*100:.1f}%)")
            
            # Analyse par proc√©dure
            if 'Proc√©dure' in cluster_data.columns:
                proc_dist = cluster_data['Proc√©dure'].value_counts().head(2)
                print(f"   ‚öôÔ∏è  Top 2 proc√©dures:")
                for proc, count in proc_dist.items():
                    print(f"      ‚Ä¢ {proc}: {count} ({count/len(cluster_data)*100:.1f}%)")
        
        # Graphique de comparaison des clusters
        self._plot_cluster_comparison(cluster_labels)
    
    def _plot_cluster_comparison(self, cluster_labels):
        """Graphique de comparaison entre clusters"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))
        
        n_clusters = len(np.unique(cluster_labels))
        colors = plt.cm.Set3(np.linspace(0, 1, n_clusters))
        
        # 1. R√©partition par r√©gion
        if 'region' in self.df.columns:
            region_cluster = pd.crosstab(self.df['region'], cluster_labels, normalize='columns') * 100
            region_cluster.plot(kind='bar', ax=ax1, color=colors, alpha=0.8)
            ax1.set_title('üåç R√©partition R√©gionale par Cluster (%)', fontsize=12, fontweight='bold')
            ax1.set_xlabel('R√©gion', fontweight='bold')
            ax1.set_ylabel('Pourcentage', fontweight='bold')
            ax1.legend(title='Cluster', title_fontsize=10)
            ax1.tick_params(axis='x', rotation=45)
        
        # 2. R√©partition par cat√©gorie
        if 'Cat√©gorie' in self.df.columns:
            cat_cluster = pd.crosstab(self.df['Cat√©gorie'], cluster_labels, normalize='columns') * 100
            cat_cluster.plot(kind='bar', ax=ax2, color=colors, alpha=0.8)
            ax2.set_title('üìÅ R√©partition par Cat√©gorie (%)', fontsize=12, fontweight='bold')
            ax2.set_xlabel('Cat√©gorie', fontweight='bold')
            ax2.set_ylabel('Pourcentage', fontweight='bold')
            ax2.legend(title='Cluster', title_fontsize=10)
            ax2.tick_params(axis='x', rotation=45)
        
        # 3. √âvolution temporelle par cluster
        if 'Ann√©e' in self.df.columns:
            temp_cluster = pd.crosstab(self.df['Ann√©e'], cluster_labels)
            temp_cluster.plot(kind='line', ax=ax3, marker='o', linewidth=2, markersize=6, color=colors)
            ax3.set_title('üìà √âvolution Temporelle par Cluster', fontsize=12, fontweight='bold')
            ax3.set_xlabel('Ann√©e', fontweight='bold')
            ax3.set_ylabel('Nombre de March√©s', fontweight='bold')
            ax3.legend(title='Cluster', title_fontsize=10)
            ax3.grid(True, alpha=0.3)
        
        # 4. R√©partition par proc√©dure
        if 'Proc√©dure' in self.df.columns:
            proc_cluster = pd.crosstab(self.df['Proc√©dure'], cluster_labels, normalize='columns') * 100
            proc_cluster.plot(kind='bar', ax=ax4, color=colors, alpha=0.8)
            ax4.set_title('‚öôÔ∏è R√©partition par Proc√©dure (%)', fontsize=12, fontweight='bold')
            ax4.set_xlabel('Proc√©dure', fontweight='bold')
            ax4.set_ylabel('Pourcentage', fontweight='bold')
            ax4.legend(title='Cluster', title_fontsize=10)
            ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
    
    def cross_analysis_improved(self):
        """Analyse crois√©e avec visualisations am√©lior√©es"""
        print("\n=== ANALYSE CROIS√âE AM√âLIOR√âE ===")
        
        # 1. Heatmap R√©gion √ó Type de d√©pense
        if 'region' in self.df.columns and 'Type de d√©pense personnalis√©' in self.df.columns:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            
            crosstab_region_type = pd.crosstab(
                self.df['region'], 
                self.df['Type de d√©pense personnalis√©']
            )
            
            # Heatmap des valeurs absolues
            sns.heatmap(crosstab_region_type, annot=True, fmt='d', cmap='YlOrRd', 
                       ax=ax1, cbar_kws={'label': 'Nombre de march√©s'})
            ax1.set_title('üî• Heatmap: R√©gion √ó Type de D√©pense\n(Valeurs absolues)', 
                         fontsize=14, fontweight='bold')
            ax1.set_xlabel('Type de D√©pense', fontweight='bold')
            ax1.set_ylabel('R√©gion', fontweight='bold')
            
            # Heatmap des pourcentages
            crosstab_pct = pd.crosstab(
                self.df['region'], 
                self.df['Type de d√©pense personnalis√©'], 
                normalize='index'
            ) * 100
            
            sns.heatmap(crosstab_pct, annot=True, fmt='.1f', cmap='Blues', 
                       ax=ax2, cbar_kws={'label': 'Pourcentage (%)'})
            ax2.set_title('üìä Heatmap: R√©gion √ó Type de D√©pense\n(Pourcentages par r√©gion)', 
                         fontsize=14, fontweight='bold')
            ax2.set_xlabel('Type de D√©pense', fontweight='bold')
            ax2.set_ylabel('R√©gion', fontweight='bold')
            
            plt.tight_layout()
            plt.show()
        
        # 2. Analyse temporelle d√©taill√©e
        if 'Ann√©e' in self.df.columns and 'region' in self.df.columns:
            self._plot_temporal_analysis()
        
        # 3. Top acheteurs par r√©gion
        if 'Acheteur public' in self.df.columns and 'region' in self.df.columns:
            self._plot_top_acheteurs()
    
    def _plot_temporal_analysis(self):
        """Analyse temporelle d√©taill√©e"""
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))
        
        # 1. √âvolution par r√©gion
        temporal_region = self.df.groupby(['Ann√©e', 'region']).size().unstack(fill_value=0)
        
        for region in temporal_region.columns:
            ax1.plot(temporal_region.index, temporal_region[region], 
                    marker='o', linewidth=2.5, markersize=6, label=region)
        
        ax1.set_title('üìà √âvolution Temporelle par R√©gion', fontsize=12, fontweight='bold')
        ax1.set_xlabel('Ann√©e', fontweight='bold')
        ax1.set_ylabel('Nombre de March√©s', fontweight='bold')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax1.grid(True, alpha=0.3)
        
        # 2. √âvolution par cat√©gorie
        if 'Cat√©gorie' in self.df.columns:
            temporal_cat = self.df.groupby(['Ann√©e', 'Cat√©gorie']).size().unstack(fill_value=0)
            temporal_cat.plot(kind='bar', ax=ax2, alpha=0.8, width=0.8)
            ax2.set_title('üìÅ √âvolution par Cat√©gorie', fontsize=12, fontweight='bold')
            ax2.set_xlabel('Ann√©e', fontweight='bold')
            ax2.set_ylabel('Nombre de March√©s', fontweight='bold')
            ax2.legend(title='Cat√©gorie', fontsize=9)
            ax2.tick_params(axis='x', rotation=45)
        
        # 3. √âvolution mensuelle (derni√®re ann√©e)
        if 'Mois' in self.df.columns:
            last_year = self.df['Ann√©e'].max()
            monthly_data = self.df[self.df['Ann√©e'] == last_year].groupby('Mois').size()
            
            ax3.bar(monthly_data.index, monthly_data.values, alpha=0.8, color='skyblue')
            ax3.set_title(f'üìÖ R√©partition Mensuelle ({last_year})', fontsize=12, fontweight='bold')
            ax3.set_xlabel('Mois', fontweight='bold')
            ax3.set_ylabel('Nombre de March√©s', fontweight='bold')
            ax3.set_xticks(range(1, 13))
            ax3.grid(True, alpha=0.3, axis='y')
        
        # 4. Croissance annuelle
        annual_growth = temporal_region.sum(axis=1).pct_change() * 100
        ax4.bar(annual_growth.index[1:], annual_growth.values[1:], 
               alpha=0.8, color=['green' if x > 0 else 'red' for x in annual_growth.values[1:]])
        ax4.set_title('üìä Croissance Annuelle (%)', fontsize=12, fontweight='bold')
        ax4.set_xlabel('Ann√©e', fontweight='bold')
        ax4.set_ylabel('Croissance (%)', fontweight='bold')
        ax4.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def _plot_top_acheteurs(self):
        """Analyse des top acheteurs par r√©gion"""
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
        
        # 1. Top 10 acheteurs globaux
        top_acheteurs = self.df['Acheteur public'].value_counts().head(10)
        
        bars1 = ax1.barh(range(len(top_acheteurs)), top_acheteurs.values, alpha=0.8, color='lightblue')
        ax1.set_yticks(range(len(top_acheteurs)))
        ax1.set_yticklabels([name[:50] + '...' if len(name) > 50 else name 
                            for name in top_acheteurs.index], fontsize=10)
        ax1.set_xlabel('Nombre de March√©s', fontweight='bold')
        ax1.set_title('üèõÔ∏è Top 10 Acheteurs Publics', fontsize=14, fontweight='bold')
        
        # Ajouter les valeurs sur les barres
        for i, (bar, value) in enumerate(zip(bars1, top_acheteurs.values)):
            ax1.text(bar.get_width() + max(top_acheteurs.values)*0.01, bar.get_y() + bar.get_height()/2, 
                    f'{value}', ha='left', va='center', fontweight='bold')
        
        # 2. R√©partition par r√©gion des top acheteurs
        acheteur_region = self.df.groupby(['region', 'Acheteur public']).size().reset_index(name='count')
        top_by_region = acheteur_region.loc[acheteur_region.groupby('region')['count'].idxmax()]
        
        bars2 = ax2.bar(range(len(top_by_region)), top_by_region['count'], alpha=0.8, color='lightcoral')
        ax2.set_xticks(range(len(top_by_region)))
        ax2.set_xticklabels(top_by_region['region'], rotation=45, ha='right')
        ax2.set_ylabel('Nombre de March√©s', fontweight='bold')
        ax2.set_title('üåç Principal Acheteur par R√©gion', fontsize=14, fontweight='bold')
        
        # Ajouter les noms des acheteurs
        for i, (bar, acheteur, count) in enumerate(zip(bars2, top_by_region['Acheteur public'], top_by_region['count'])):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(top_by_region['count'])*0.01, 
                    f'{count}\n{acheteur[:20]}...', ha='center', va='bottom', fontsize=8, rotation=0)
        
        plt.tight_layout()
        plt.show()
    
    def generate_comprehensive_report(self):
        """Rapport de synth√®se complet et d√©taill√©"""
        print("\n" + "="*80)
        print("                    RAPPORT DE SYNTH√àSE COMPLET")
        print("               Analyse Statistique des March√©s Publics Marocains")
        print("                           Version Am√©lior√©e 2025")
        print("="*80)
        
        # Statistiques g√©n√©rales
        print(f"\nüìä STATISTIQUES G√âN√âRALES:")
        print(f"   ‚Ä¢ Nombre total de march√©s analys√©s: {len(self.df):,}")
        print(f"   ‚Ä¢ P√©riode d'analyse: {self.df['Ann√©e'].min():.0f} - {self.df['Ann√©e'].max():.0f}")
        print(f"   ‚Ä¢ Nombre de r√©gions couvertes: {self.df['region'].nunique()}")
        print(f"   ‚Ä¢ Nombre d'acheteurs publics uniques: {self.df['Acheteur public'].nunique()}")
        
        # Analyse ACP
        if self.pca is not None:
            print(f"\nüîç R√âSULTATS DE L'ANALYSE EN COMPOSANTES PRINCIPALES:")
            print(f"   ‚Ä¢ Variance expliqu√©e par la 1√®re composante: {self.pca.explained_variance_ratio_[0]:.1%}")
            print(f"   ‚Ä¢ Variance expliqu√©e par les 2 premi√®res: {sum(self.pca.explained_variance_ratio_[:2]):.1%}")
            print(f"   ‚Ä¢ Variance expliqu√©e par les 3 premi√®res: {sum(self.pca.explained_variance_ratio_[:3]):.1%}")
            print(f"   ‚Ä¢ Nombre de composantes pour 80% de variance: {sum(self.pca.explained_variance_ratio_.cumsum() < 0.8) + 1}")
            print(f"   ‚Ä¢ Dimensionnalit√© r√©duite de {self.df_encoded.shape[1]} √† {len(self.pca.explained_variance_ratio_)} variables")
        
        # Analyse clustering
        if 'Cluster_KMeans' in self.df.columns:
            cluster_counts = self.df['Cluster_KMeans'].value_counts().sort_index()
            print(f"\nüéØ R√âSULTATS DU CLUSTERING K-MEANS:")
            print(f"   ‚Ä¢ Nombre optimal de clusters identifi√©s: {len(cluster_counts)}")
            print(f"   ‚Ä¢ R√©partition des march√©s par cluster:")
            for cluster, count in cluster_counts.items():
                print(f"     - Cluster {cluster}: {count:,} march√©s ({count/len(self.df)*100:.1f}%)")
            
            # Silhouette score
            if hasattr(self, 'final_silhouette_score'):
                print(f"   ‚Ä¢ Score de silhouette final: {self.final_silhouette_score:.3f}")
        
        # R√©partition par cat√©gorie
        print(f"\nüìÅ R√âPARTITION PAR CAT√âGORIE:")
        cat_counts = self.df['Cat√©gorie'].value_counts()
        for cat, count in cat_counts.items():
            print(f"   ‚Ä¢ {cat}: {count:,} march√©s ({count/len(self.df)*100:.1f}%)")
        
        # R√©partition par r√©gion
        print(f"\nüåç R√âPARTITION G√âOGRAPHIQUE:")
        region_counts = self.df['region'].value_counts().head(5)
        print("   Top 5 des r√©gions les plus actives:")
        for region, count in region_counts.items():
            print(f"   ‚Ä¢ {region}: {count:,} march√©s ({count/len(self.df)*100:.1f}%)")
        
        # Top acheteurs
        print(f"\nüèõÔ∏è PRINCIPAUX ACHETEURS PUBLICS:")
        top_acheteurs = self.df['Acheteur public'].value_counts().head(5)
        for i, (acheteur, count) in enumerate(top_acheteurs.items(), 1):
            acheteur_display = acheteur[:60] + "..." if len(acheteur) > 60 else acheteur
            print(f"   {i}. {acheteur_display}")
            print(f"      ‚Üí {count:,} march√©s ({count/len(self.df)*100:.1f}%)")
        
        # Analyse temporelle
        if 'Ann√©e' in self.df.columns:
            annual_dist = self.df['Ann√©e'].value_counts().sort_index()
            print(f"\nüìà √âVOLUTION TEMPORELLE:")
            print("   R√©partition par ann√©e:")
            for year, count in annual_dist.items():
                growth = ""
                if year > annual_dist.index.min():
                    prev_count = annual_dist.get(year-1, 0)
                    if prev_count > 0:
                        growth_rate = ((count - prev_count) / prev_count) * 100
                        growth = f" ({growth_rate:+.1f}%)"
                print(f"   ‚Ä¢ {year:.0f}: {count:,} march√©s{growth}")
        
        # Insights et recommandations
        print(f"\nüí° INSIGHTS PRINCIPAUX:")
        
        # Concentration g√©ographique
        top_3_regions_pct = (self.df['region'].value_counts().head(3).sum() / len(self.df)) * 100
        print(f"   ‚Ä¢ Concentration g√©ographique: Les 3 premi√®res r√©gions repr√©sentent {top_3_regions_pct:.1f}% des march√©s")
        
        # Concentration des acheteurs
        top_10_acheteurs_pct = (self.df['Acheteur public'].value_counts().head(10).sum() / len(self.df)) * 100
        print(f"   ‚Ä¢ Concentration des acheteurs: Les 10 premiers acheteurs repr√©sentent {top_10_acheteurs_pct:.1f}% des march√©s")
        
        # Diversit√© des proc√©dures
        if 'Proc√©dure' in self.df.columns:
            proc_diversity = self.df['Proc√©dure'].nunique()
            print(f"   ‚Ä¢ Diversit√© des proc√©dures: {proc_diversity} types de proc√©dures utilis√©es")
        
        print(f"\nüéØ RECOMMANDATIONS:")
        print("   ‚Ä¢ Analyser les facteurs de concentration g√©ographique")
        print("   ‚Ä¢ √âtudier les patterns de clustering pour optimiser les processus")
        print("   ‚Ä¢ Surveiller l'√©volution temporelle pour anticiper les tendances")
        print("   ‚Ä¢ Approfondir l'analyse des acheteurs les plus actifs")
        
        print("\n" + "="*80)
        print("                          FIN DU RAPPORT")
        print("="*80)

# Fonction principale am√©lior√©e
def main_improved():
    """Fonction principale pour ex√©cuter l'analyse compl√®te am√©lior√©e"""
    
    print("üöÄ ANALYSE STATISTIQUE AVANC√âE - VERSION AM√âLIOR√âE")
    print("üìã March√©s Publics Marocains - Graphiques Optimis√©s")
    print("-" * 70)
    
    try:
        # Chargement des donn√©es (remplacez par votre fichier)
        df = pd.read_excel('C:/Users/lenovo/Downloads/stage_Application MP/base_de_donnee_finale_encodee.xlsx')
        
        # Cr√©ation de l'analyseur am√©lior√©
        analyser = AnalyseMarchesPublicsAmeliore(df=df)
        
        print("üîß Phase 1: Pr√©paration des donn√©es...")
        analyser.preprocess_data()
        
        print("üìä Phase 2: Analyse en Composantes Principales...")
        X_pca = analyser.perform_pca(n_components=10)
        
        print("üéØ Phase 3: Clustering K-means...")
        cluster_labels = analyser.perform_clustering(method='kmeans', n_clusters_range=(2, 8))
        
        print("üîÄ Phase 4: Analyses crois√©es...")
        analyser.cross_analysis_improved()
        
        print("üìù Phase 5: G√©n√©ration du rapport final...")
        analyser.generate_comprehensive_report()
        
        print("\n‚úÖ ANALYSE TERMIN√âE AVEC SUCC√àS!")
        print("üìä Tous les graphiques ont √©t√© affich√©s sans chevauchement.")
        print("üìã Consultez le rapport d√©taill√© ci-dessus.")
        
        return analyser
        
    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse: {e}")
        print("üí° V√©rifiez le chemin du fichier et les colonnes requises.")
        return None

# Instructions d'utilisation am√©lior√©es
def print_improved_instructions():
    """Instructions d√©taill√©es pour la version am√©lior√©e"""
    
    instructions = """
    üìã GUIDE D'UTILISATION - VERSION AM√âLIOR√âE
    ==========================================
    
    üéØ NOUVEAUT√âS DE CETTE VERSION:
    ‚Ä¢ Graphiques s√©par√©s sans chevauchement
    ‚Ä¢ Visualisations am√©lior√©es avec l√©gendes claires
    ‚Ä¢ Analyses plus d√©taill√©es avec interpr√©tations
    ‚Ä¢ Rapport de synth√®se complet
    ‚Ä¢ Code optimis√© et comment√©
    
    üîß UTILISATION RAPIDE:
    1. Remplacez le chemin du fichier dans main_improved()
    2. Ex√©cutez: analyser = main_improved()
    3. Les graphiques s'affichent automatiquement sans chevauchement
    
    üìä GRAPHIQUES G√âN√âR√âS:
    ‚Ä¢ ACP: √âboulis + Variance cumul√©e (s√©par√©s)
    ‚Ä¢ ACP: Projections + Cercle corr√©lations (s√©par√©s)
    ‚Ä¢ Clustering: Coude + Silhouette (s√©par√©s)
    ‚Ä¢ Clustering: R√©sultats + Distribution (s√©par√©s)
    ‚Ä¢ Analyses crois√©es: Heatmaps et comparaisons
    
    üí° CONSEILS:
    ‚Ä¢ Ajustez les param√®tres selon vos besoins
    ‚Ä¢ Sauvegardez les graphiques avec plt.savefig()
    ‚Ä¢ Exportez le rapport dans un fichier texte
    """
    
    print(instructions)

# Test avec donn√©es d'exemple
def test_improved():
    """Test rapide de la version am√©lior√©e"""
    
    # Donn√©es d'exemple plus r√©alistes
    np.random.seed(42)
    n_samples = 1000
    
    sample_data = {
        'R√©f√©rence': pd.date_range('2021-01-01', periods=n_samples, freq='D'),
        'Proc√©dure': np.random.choice(['AOO', 'AOS', 'APPEL'], n_samples, p=[0.6, 0.3, 0.1]),
        'Cat√©gorie': np.random.choice(['Services', 'Travaux', 'Fournitures'], n_samples, p=[0.5, 0.3, 0.2]),
        'Type de d√©pense personnalis√©': np.random.choice(['Autre', 'Transport', 'Informatique', 'Maintenance'], n_samples),
        'region': np.random.choice(['Casablanca-Settat', 'Souss-Massa', 'Oriental', 'Rabat-Sal√©'], n_samples),
        'Acheteur public': np.random.choice([f'Minist√®re {i}' for i in range(1, 21)], n_samples),
        'Procedure_AOO': np.random.choice([0, 1], n_samples),
        'Procedure_AOS': np.random.choice([0, 1], n_samples),
        'Categorie_Services': np.random.choice([0, 1], n_samples),
        'Categorie_Travaux': np.random.choice([0, 1], n_samples),
        'Categorie_Fournitures': np.random.choice([0, 1], n_samples)
    }
    
    df_test = pd.DataFrame(sample_data)
    
    print("üß™ TEST DE LA VERSION AM√âLIOR√âE")
    print("-" * 50)
    
    try:
        analyser = AnalyseMarchesPublicsAmeliore(df=df_test)
        
        analyser.preprocess_data()
        analyser.perform_pca(n_components=8)
        analyser.perform_clustering(method='kmeans', n_clusters_range=(2, 6))
        analyser.cross_analysis_improved()
        analyser.generate_comprehensive_report()
        
        print("‚úÖ Test r√©ussi! Graphiques affich√©s sans chevauchement.")
        return analyser
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test: {e}")
        return None
analyser = main_improved()
if __name__ == "__main__":
    print_improved_instructions()
    
    # D√©commentez pour ex√©cuter:
    # analyser = main_improved()  # Analyse avec vos vraies donn√©es
    # analyser = test_improved()  # Test rapide avec donn√©es simul√©es
